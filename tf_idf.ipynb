{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec73f863-2ff6-426c-bed9-27025112baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778d8c89-75b3-4a6a-ada4-26bc638cae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat    chased       dog       log       mat     mouse        on  \\\n",
      "0  0.374207  0.000000  0.000000  0.000000  0.492038  0.000000  0.374207   \n",
      "1  0.000000  0.000000  0.468699  0.468699  0.000000  0.000000  0.356457   \n",
      "2  0.381519  0.501651  0.000000  0.000000  0.000000  0.501651  0.000000   \n",
      "\n",
      "        sat       the  \n",
      "0  0.374207  0.581211  \n",
      "1  0.356457  0.553642  \n",
      "2  0.000000  0.592567  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the mouse\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to dense matrix and print\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf77974-95f4-43e7-89cd-712ab116ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¸ Count Vectorizer Output:\n",
      "   cat  chased  dog  log  mat  mouse  on  sat  the\n",
      "0    1       0    0    0    1      0   1    1    2\n",
      "1    0       0    1    1    0      0   1    1    2\n",
      "2    1       1    0    0    0      1   0    0    2\n",
      "\n",
      "ğŸ”¹ TF-IDF Vectorizer Output:\n",
      "    cat  chased   dog   log   mat  mouse    on   sat   the\n",
      "0  0.37     0.0  0.00  0.00  0.49    0.0  0.37  0.37  0.58\n",
      "1  0.00     0.0  0.47  0.47  0.00    0.0  0.36  0.36  0.55\n",
      "2  0.38     0.5  0.00  0.00  0.00    0.5  0.00  0.00  0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the mouse\"\n",
    "]\n",
    "\n",
    "# ------------------------------\n",
    "# CountVectorizer\n",
    "# ------------------------------\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "print(\"ğŸ”¸ Count Vectorizer Output:\")\n",
    "print(count_df)\n",
    "\n",
    "# ------------------------------\n",
    "# TfidfVectorizer\n",
    "# ------------------------------\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nğŸ”¹ TF-IDF Vectorizer Output:\")\n",
    "print(tfidf_df.round(2))  # Rounded for easier comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc39ba0-e3dc-49fb-bdca-a2ff5294fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¸ CountVectorizer Output:\n",
      "   across  ai  algorithms  all  and  applications  are  artificial  \\\n",
      "0       1   1           0    0    1             0    1           1   \n",
      "1       0   1           0    0    1             0    0           1   \n",
      "2       0   0           1    1    1             1    0           1   \n",
      "\n",
      "   assistants  autonomous  ...  society  subset  systems  the  to  together  \\\n",
      "0           0           0  ...        0       0        1    1   1         0   \n",
      "1           0           0  ...        1       0        0    1   1         1   \n",
      "2           1           1  ...        0       1        1    0   0         0   \n",
      "\n",
      "   transforming  vehicles  voice  work  \n",
      "0             1         0      0     0  \n",
      "1             0         0      0     1  \n",
      "2             0         1      1     0  \n",
      "\n",
      "[3 rows x 53 columns] \n",
      "\n",
      "ğŸ”¹ TfidfVectorizer Output:\n",
      "   across    ai  algorithms  all   and  applications   are  artificial  \\\n",
      "0    0.24  0.18         0.0  0.0  0.14           0.0  0.24        0.14   \n",
      "1    0.00  0.18         0.0  0.0  0.14           0.0  0.00        0.14   \n",
      "2    0.00  0.00         0.2  0.2  0.12           0.2  0.00        0.12   \n",
      "\n",
      "   assistants  autonomous  ...  society  subset  systems   the    to  \\\n",
      "0         0.0         0.0  ...     0.00     0.0     0.18  0.18  0.18   \n",
      "1         0.0         0.0  ...     0.23     0.0     0.00  0.18  0.18   \n",
      "2         0.2         0.2  ...     0.00     0.2     0.15  0.00  0.00   \n",
      "\n",
      "   together  transforming  vehicles  voice  work  \n",
      "0      0.00          0.24       0.0    0.0  0.00  \n",
      "1      0.23          0.00       0.0    0.0  0.23  \n",
      "2      0.00          0.00       0.2    0.2  0.00  \n",
      "\n",
      "[3 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Larger text documents\n",
    "documents = [\n",
    "    \"\"\"Artificial intelligence is transforming industries across the globe. \n",
    "    From healthcare to finance, AI systems are enhancing efficiency and decision-making processes.\"\"\",\n",
    "\n",
    "    \"\"\"The rapid development of artificial intelligence raises important ethical questions. \n",
    "    Governments and organizations must work together to ensure AI benefits society fairly.\"\"\",\n",
    "\n",
    "    \"\"\"Machine learning, a subset of artificial intelligence, is powering many everyday applications. \n",
    "    Voice assistants, recommendation systems, and autonomous vehicles all rely on machine learning algorithms.\"\"\"\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Count Vectorizer\n",
    "# -------------------------\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# -------------------------\n",
    "# TF-IDF Vectorizer\n",
    "# -------------------------\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display results\n",
    "print(\"ğŸ”¸ CountVectorizer Output:\")\n",
    "print(count_df.head(), \"\\n\")\n",
    "\n",
    "print(\"ğŸ”¹ TfidfVectorizer Output:\")\n",
    "print(tfidf_df.round(2).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275b1c8e-f4b6-4089-a75d-ad385c87da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Cosine Similarity Matrix:\n",
      "      Doc1  Doc2  Doc3  Doc4\n",
      "Doc1  1.00  1.00  0.00  0.06\n",
      "Doc2  1.00  1.00  0.00  0.06\n",
      "Doc3  0.00  0.00  1.00  0.22\n",
      "Doc4  0.06  0.06  0.22  1.00\n",
      "\n",
      "âš ï¸ Potential Plagiarism Cases (similarity > 0.8):\n",
      " - Doc1 and Doc2 are 1.00 similar.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents (simulate student submissions or papers)\n",
    "documents = [\n",
    "    \"Artificial intelligence is transforming many industries and changing how we work.\",\n",
    "    \"Artificial intelligence is changing how we work and transforming many industries.\",\n",
    "    \"The history of AI dates back to the 1950s when the field was just beginning.\",\n",
    "    \"This is a completely different paragraph unrelated to the others.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectors for all documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Compute cosine similarity between every pair of documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "\n",
    "doc_labels = [f\"Doc{i+1}\" for i in range(len(documents))]\n",
    "df = pd.DataFrame(similarity_matrix, index=doc_labels, columns=doc_labels)\n",
    "\n",
    "print(\"ğŸ” Cosine Similarity Matrix:\")\n",
    "print(df.round(2))\n",
    "\n",
    "# Threshold to flag as plagiarism\n",
    "threshold = 0.8\n",
    "print(\"\\nâš ï¸ Potential Plagiarism Cases (similarity > 0.8):\")\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i + 1, len(documents)):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        if sim > threshold:\n",
    "            print(f\" - {doc_labels[i]} and {doc_labels[j]} are {sim:.2f} similar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7371f064-433a-4a7b-be3f-5d6abfe92a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” TF-IDF Matrix (Lemmatized, simple tokenizer):\n",
      "               Doc1   Doc2   Doc3   Doc4   Doc5\n",
      "across        0.350  0.000  0.000  0.000  0.000\n",
      "artificial    0.167  0.143  0.143  0.143  0.125\n",
      "changing      0.000  0.000  0.242  0.242  0.000\n",
      "ethical       0.000  0.300  0.000  0.000  0.000\n",
      "globe         0.350  0.000  0.000  0.000  0.000\n",
      "important     0.000  0.300  0.000  0.000  0.000\n",
      "industry      0.234  0.000  0.201  0.201  0.000\n",
      "intelligence  0.167  0.143  0.143  0.143  0.125\n",
      "learning      0.000  0.000  0.000  0.000  0.262\n",
      "machine       0.000  0.000  0.000  0.000  0.262\n",
      "many          0.000  0.000  0.201  0.201  0.176\n",
      "question      0.000  0.300  0.000  0.000  0.000\n",
      "raise         0.000  0.300  0.000  0.000  0.000\n",
      "society       0.000  0.300  0.000  0.000  0.000\n",
      "subset        0.000  0.000  0.000  0.000  0.262\n",
      "system        0.000  0.000  0.000  0.000  0.262\n",
      "transforming  0.234  0.000  0.201  0.201  0.000\n",
      "used          0.000  0.000  0.000  0.000  0.262\n",
      "work          0.000  0.000  0.242  0.242  0.000\n",
      "\n",
      "ğŸ” Document Similarity (Cosine Similarity based on TF-IDF):\n",
      "       Doc1   Doc2   Doc3   Doc4   Doc5\n",
      "Doc1  1.000  0.106  0.419  0.419  0.102\n",
      "Doc2  0.106  1.000  0.110  0.110  0.080\n",
      "Doc3  0.419  0.110  1.000  1.000  0.211\n",
      "Doc4  0.419  0.110  1.000  1.000  0.211\n",
      "Doc5  0.102  0.080  0.211  0.211  1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ehmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ehmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download only needed NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Simple tokenizer (no punkt dependency)\n",
    "def simple_tokenize(text):\n",
    "    # Extract words (alphanumeric sequences)\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = simple_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # remove stopwords\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]  # lemmatize\n",
    "    return lemmas\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Artificial intelligence is transforming industries across the globe.\",\n",
    "    \"Artificial intelligence raises important ethical questions for society.\",\n",
    "    \"Artificial intelligence is transforming many industries and changing how we work.\",\n",
    "    \"Artificial intelligence is changing how we work and transforming many industries.\",\n",
    "    \"Machine learning is a subset of artificial intelligence used in many systems.\"\n",
    "]\n",
    "\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(word for doc in processed_docs for word in doc))\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "def compute_tf(doc):\n",
    "    tf_dict = {}\n",
    "    word_count = len(doc)\n",
    "    counts = Counter(doc)\n",
    "    for word in vocab:\n",
    "        tf_dict[word] = counts[word] / word_count if word_count > 0 else 0\n",
    "    return tf_dict\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "def compute_idf(docs):\n",
    "    idf_dict = {}\n",
    "    total_docs = len(docs)\n",
    "    for word in vocab:\n",
    "        doc_count = sum(1 for doc in docs if word in doc)\n",
    "        idf_dict[word] = math.log((total_docs + 1) / (doc_count + 1)) + 1  # smoothed IDF\n",
    "    return idf_dict\n",
    "\n",
    "# Compute TF-IDF for all docs\n",
    "tf_list = [compute_tf(doc) for doc in processed_docs]\n",
    "idf = compute_idf(processed_docs)\n",
    "\n",
    "tfidf_docs = []\n",
    "for tf_doc in tf_list:\n",
    "    tfidf_doc = {word: tf_doc[word] * idf[word] for word in vocab}\n",
    "    tfidf_docs.append(tfidf_doc)\n",
    "\n",
    "# Display TF-IDF matrix as a DataFrame\n",
    "df = pd.DataFrame(tfidf_docs).T\n",
    "df.columns = [f'Doc{i+1}' for i in range(len(documents))]\n",
    "\n",
    "print(\"ğŸ” TF-IDF Matrix (Lemmatized, simple tokenizer):\")\n",
    "print(df.round(3))\n",
    "tfidf_matrix = np.array([[tfidf_doc[word] for word in vocab] for tfidf_doc in tfidf_docs])\n",
    "\n",
    "# Compute cosine similarity between documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity matrix\n",
    "sim_df = pd.DataFrame(similarity_matrix, \n",
    "                      index=[f'Doc{i+1}' for i in range(len(documents))], \n",
    "                      columns=[f'Doc{i+1}' for i in range(len(documents))])\n",
    "\n",
    "print(\"\\nğŸ” Document Similarity (Cosine Similarity based on TF-IDF):\")\n",
    "print(sim_df.round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f89d930-ff1c-4268-ba32-2fcd62203309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "texts = [\"free money now\", \"hello how are you\", \"win big cash\", \"meeting tomorrow\"]\n",
    "labels = [\"spam\", \"ham\", \"spam\", \"ham\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Predict\n",
    "new_text = [\"free cash offer\"]\n",
    "new_X = vectorizer.transform(new_text)\n",
    "print(model.predict(new_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702808-9886-47d3-94f4-be189335cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b8011-90ae-4b10-ba4c-2a1acae456b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f6acd918-07c2-48e3-94d9-f80e5de60a3f",
   "metadata": {},
   "source": [
    "1. AÅPA hÉ™qiqÉ™tlÉ™ri gÃ¶rmÃ¼r: ErmÉ™nistanda mediaya hÃ¼cumlara gÃ¶z yumulur - ÅÆRH + FOTO\n",
    "2. A.MÉ™likli S.BaÄŸÄ±rova deyib ki, â€œBiz bu mÉ™lumatlara 154.5$ pul xÉ™rclÉ™miÅŸik.\n",
    "3. â€œÄ°nformasiyanÄ± yaymaq hÃ¼ququnun pozulmasÄ± ilÉ™ baÄŸlÄ± 2023-cÃ¼ ildÉ™ 135 fakt qeydÉ™ alÄ±nÄ±b. \n",
    "4. ÆdÉ™dlÉ™r nÉ™zÉ™riyyÉ™sinin inkiÅŸafÄ± É™dÉ™dlÉ™r, yÉ™ni natural (N), tam (Z) vÉ™ rasional (Q) É™dÉ™dlÉ™r Ã¼zÉ™rindÉ™ki É™mÉ™llÉ™rlÉ™ baÅŸladÄ±.\n",
    "5. XÉ™bÉ™rlÉ™rin 20:00 buraxÄ±lÄ±ÅŸÄ±\n",
    "6. BÃ¶yÃ¼k SÉ™lcuq imperatorluÄŸu faktiki olaraq ayrÄ± â€“ ayrÄ± dÃ¶vlÉ™tlÉ™rÉ™ parÃ§alandÄ±:\n",
    "7. a=5,1; b=2.9 olduqda a2-4a+ab-4b ifadÉ™sinin qiymÉ™tini tapÄ±n.\n",
    "8. Kater Ã§ay axÄ±nÄ± ilÉ™ 16 km/saat, axÄ±na qarÅŸÄ± isÉ™ 10km/saat sÃ¼rÉ™tlÉ™ gedir.\n",
    "9. ÆdÉ™di 2,5 dÉ™fÉ™ artÄ±rsaq, É™dÉ™d neÃ§É™ faiz artar?\n",
    "10. ÆgÉ™r malÄ±n qiymÉ™ti: a) 40% ucuzlaÅŸarsa; b)50.5 % bahalaÅŸarsa; malÄ±n indiki qiymÉ™tiÂ neÃ§É™Â faizÂ olar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6993cb8-89d5-47fe-9c08-d295253977af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', 'AÅPA', 'hÉ™qiqÉ™tlÉ™ri', 'gÃ¶rmÃ¼r', ':', 'ErmÉ™nistanda', 'mediaya', 'hÃ¼cumlara', 'gÃ¶z', 'yumulur', '-', 'ÅÆRH', '+', 'FOTO', '2', '.', 'A', '.', 'MÉ™likli', 'S', '.', 'BaÄŸÄ±rova', 'deyib', 'ki', ',', 'â€œ', 'Biz', 'bu', 'mÉ™lumatlara', '154.5', '$', 'pul', 'xÉ™rclÉ™miÅŸik', '.', '3', '.', 'â€œ', 'Ä°nformasiyanÄ±', 'yaymaq', 'hÃ¼ququnun', 'pozulmasÄ±', 'ilÉ™', 'baÄŸlÄ±', '2023', '-', 'cÃ¼', 'ildÉ™', '135', 'fakt', 'qeydÉ™', 'alÄ±nÄ±b', '.', '4', '.', 'ÆdÉ™dlÉ™r', 'nÉ™zÉ™riyyÉ™sinin', 'inkiÅŸafÄ±', 'É™dÉ™dlÉ™r', ',', 'yÉ™ni', 'natural', '(', 'N', ')', ',', 'tam', '(', 'Z', ')', 'vÉ™', 'rasional', '(', 'Q', ')', 'É™dÉ™dlÉ™r', 'Ã¼zÉ™rindÉ™ki', 'É™mÉ™llÉ™rlÉ™', 'baÅŸladÄ±', '.', '5', '.', 'XÉ™bÉ™rlÉ™rin', '20', ':', '00', 'buraxÄ±lÄ±ÅŸÄ±', '6', '.', 'BÃ¶yÃ¼k', 'SÉ™lcuq', 'imperatorluÄŸu', 'faktiki', 'olaraq', 'ayrÄ±', 'â€“', 'ayrÄ±', 'dÃ¶vlÉ™tlÉ™rÉ™', 'parÃ§alandÄ±', ':', '7', '.', 'a', '=', '5,1', ';', 'b', '=', '2.9', 'olduqda', 'a2', '-', '4', 'a', '+', 'ab', '-', '4', 'b', 'ifadÉ™sinin', 'qiymÉ™tini', 'tapÄ±n', '.', '8', '.', 'Kater', 'Ã§ay', 'axÄ±nÄ±', 'ilÉ™', '16', 'km', '/', 'saat', ',', 'axÄ±na', 'qarÅŸÄ±', 'isÉ™', '10', 'km', '/', 'saat', 'sÃ¼rÉ™tlÉ™', 'gedir', '.', '9', '.', 'ÆdÉ™di', '2,5', 'dÉ™fÉ™', 'artÄ±rsaq', ',', 'É™dÉ™d', 'neÃ§É™', 'faiz', 'artar', '?', '10', '.', 'ÆgÉ™r', 'malÄ±n', 'qiymÉ™ti', ':', 'a', ')', '40', '%', 'ucuzlaÅŸarsa', ';', 'b', ')', '50.5', '%', 'bahalaÅŸarsa', ';', 'malÄ±n', 'indiki', 'qiymÉ™ti', 'neÃ§É™', 'faiz', 'olar', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    # Pattern explanation:\n",
    "    # \\d+,\\d+ or \\d+\\.\\d+  -> decimal numbers with comma or dot (e.g. 2,5 or 2.9)\n",
    "    # \\d+                  -> integers\n",
    "    # \\w+                  -> words (letters, digits, underscores)\n",
    "    # [%$â‚¬]                 -> symbols like % $ â‚¬\n",
    "    # [=+\\-*/()]            -> math operators and parentheses\n",
    "    # [^\\w\\s]               -> any punctuation or symbol not matched above (like :, ;, etc.)\n",
    "    \n",
    "    pattern = r'\\d+,\\d+|\\d+\\.\\d+|\\d+|\\w+|[%$â‚¬]|[=+\\-*/()]|[^\\w\\s]'\n",
    "    \n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens\n",
    "\n",
    "# Your text examples (combined into one string)\n",
    "text = \"\"\"\n",
    "1. AÅPA hÉ™qiqÉ™tlÉ™ri gÃ¶rmÃ¼r: ErmÉ™nistanda mediaya hÃ¼cumlara gÃ¶z yumulur - ÅÆRH + FOTO\n",
    "2. A.MÉ™likli S.BaÄŸÄ±rova deyib ki, â€œBiz bu mÉ™lumatlara 154.5$ pul xÉ™rclÉ™miÅŸik.\n",
    "3. â€œÄ°nformasiyanÄ± yaymaq hÃ¼ququnun pozulmasÄ± ilÉ™ baÄŸlÄ± 2023-cÃ¼ ildÉ™ 135 fakt qeydÉ™ alÄ±nÄ±b. \n",
    "4. ÆdÉ™dlÉ™r nÉ™zÉ™riyyÉ™sinin inkiÅŸafÄ± É™dÉ™dlÉ™r, yÉ™ni natural (N), tam (Z) vÉ™ rasional (Q) É™dÉ™dlÉ™r Ã¼zÉ™rindÉ™ki É™mÉ™llÉ™rlÉ™ baÅŸladÄ±.\n",
    "5. XÉ™bÉ™rlÉ™rin 20:00 buraxÄ±lÄ±ÅŸÄ±\n",
    "6. BÃ¶yÃ¼k SÉ™lcuq imperatorluÄŸu faktiki olaraq ayrÄ± â€“ ayrÄ± dÃ¶vlÉ™tlÉ™rÉ™ parÃ§alandÄ±:\n",
    "7. a=5,1; b=2.9 olduqda a2-4a+ab-4b ifadÉ™sinin qiymÉ™tini tapÄ±n.\n",
    "8. Kater Ã§ay axÄ±nÄ± ilÉ™ 16 km/saat, axÄ±na qarÅŸÄ± isÉ™ 10km/saat sÃ¼rÉ™tlÉ™ gedir.\n",
    "9. ÆdÉ™di 2,5 dÉ™fÉ™ artÄ±rsaq, É™dÉ™d neÃ§É™ faiz artar?\n",
    "10. ÆgÉ™r malÄ±n qiymÉ™ti: a) 40% ucuzlaÅŸarsa; b)50.5 % bahalaÅŸarsa; malÄ±n indiki qiymÉ™ti neÃ§É™ faiz olar?\n",
    "\"\"\"\n",
    "\n",
    "tokens = custom_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2c39a-afab-4c83-95a1-91fe03e8aeed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
